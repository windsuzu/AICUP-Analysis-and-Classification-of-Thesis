{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AICUP-USAGE.ipynb","provenance":[{"file_id":"1_oe_e4lPHWgUr5gnEYWCXeXXGm3DrLPL","timestamp":1577771035868},{"file_id":"1eHNY49IgBA6I8qSd8lipn5Un0BC2Mn3Y","timestamp":1577595886651},{"file_id":"1EBpL5Pn1gQpRjORtXIGmUzJyN4egCiHg","timestamp":1577289694523},{"file_id":"1SguLI5-mtrMhp2MnJh6QogUBLSqb1RtT","timestamp":1577251048121},{"file_id":"1Fd_Ux44Y1TUcpK851JjdTetKYOvGBhGN","timestamp":1577030337593},{"file_id":"1fFnkwPT6YVw54Qu7WH1rRj59W8lL5tbf","timestamp":1574738935690}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qB0MuUSRVC1b","colab_type":"text"},"source":["# Add Frameworks and Libraries"]},{"cell_type":"code","metadata":{"id":"rq96ri3_VE3Y","colab_type":"code","colab":{}},"source":["# Import libraries\n","import os\n","import pandas as pd\n","import pickle\n","import nltk\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import json\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_sample_weight\n","from multiprocessing import Pool\n","from nltk.tokenize import word_tokenize\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from google.colab import drive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pdc-jMY6VUhA","colab_type":"text"},"source":["# Setup Environments"]},{"cell_type":"code","metadata":{"id":"YE87dQFJVZjB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"dbabb4e4-c45f-4968-8e15-26771f575b45","executionInfo":{"status":"ok","timestamp":1578550155493,"user_tz":-480,"elapsed":26386,"user":{"displayName":"NE6081022王士杰","photoUrl":"","userId":"16413260345102275028"}}},"source":["# nltk, plt setup\n","nltk.download('punkt')\n","%matplotlib inline\n","\n","# drive setup\n","drive.mount('/content/drive')\n","dataset_url = \"/content/drive/My Drive/NCKUDMPH2/task1/dataset/\"\n","program_url = \"/content/drive/My Drive/NCKUDMPH2/task1/program/\"\n","\n","# setup gpu\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qpvtTRaUgPAA","colab_type":"text"},"source":["# Load Abstract Dataset"]},{"cell_type":"code","metadata":{"id":"OGrzope_fCvX","colab_type":"code","colab":{}},"source":["abstract = \"\"\"With the emergence of mobile and wearable devices, push notification becomes a powerful tool to connect and maintain the relationship with app users, but sending inappropriate or too many messages at the wrong time may result in the app being removed by the users. In order to maintain the retention rate and the delivery rate of advertisement, we adopt deep neural network (DNN) to develop a pop-up recommendation system “Click-sequence-aware deeP neural network (DNN)-based Pop-uPs recOmmendation (C-3PO)” enabled by collaborative filtering-based hybrid user behavioral analysis. We further verified the system with real data collected from the product security master, clean master, and CM browser, supported by Leopard Mobile Inc. (Cheetah Mobile Taiwan Agency). In this way, we can know precisely about users’ preference and frequency to click on the push notification/pop-ups, decrease the troublesome to users efficiently, and meanwhile increase the click-through rate of push notifications/pop-ups.\"\"\"\n","\n","sent_text = pd.Series(nltk.sent_tokenize(abstract))\n","testData = pd.DataFrame(sent_text, columns=['Abstract'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XvF2AOU2bd4X","colab_type":"text"},"source":["# Setup Hyperparameters"]},{"cell_type":"code","metadata":{"id":"d6AZ522ebdl6","colab_type":"code","colab":{}},"source":["embedding_dim = 300\n","hidden_dim = 512\n","learning_rate = 1e-4\n","max_epoch = 15\n","batch_size = 64"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"syVkcoWOcChU","colab_type":"text"},"source":["# Helpers"]},{"cell_type":"markdown","metadata":{"id":"O8IKI4m5cxb-","colab_type":"text"},"source":["## Tokenize Helpers"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cB-mIxF9gXly","colab":{}},"source":["# tokenize the words\n","def collect_words(data_path, n_workers=4):\n","    df = pd.read_csv(data_path, dtype=str)\n","        \n","    sent_list = []\n","    for i in df.iterrows():\n","        sent_list += i[1]['Abstract'].split('$$$')\n","\n","    chunks = [\n","        ' '.join(sent_list[i:i + len(sent_list) // n_workers])\n","        for i in range(0, len(sent_list), len(sent_list) // n_workers)\n","    ]\n","    with Pool(n_workers) as pool:\n","        chunks = pool.map_async(word_tokenize, chunks)\n","        words = set(sum(chunks.get(), []))\n","\n","    return words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uLesPmucjZb","colab_type":"text"},"source":["## Data Formatting Helpers"]},{"cell_type":"code","metadata":{"id":"99_VaWlucllE","colab_type":"code","colab":{}},"source":["def label_to_onehot(labels):\n","    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n","    onehot = [0,0,0,0,0,0]\n","    for l in labels.split('/'):\n","        onehot[label_dict[l]] = 1\n","    return onehot\n","\n","def sentence_to_indices(sentence, word_dict):\n","    return [word_dict.get(word,UNK_TOKEN) for word in word_tokenize(sentence)]\n","\n","def get_dataset(data_path, word_dict, n_workers=4):\n","    dataset = testData\n","\n","    results = [None] * n_workers\n","    with Pool(processes=n_workers) as pool:\n","        for i in range(n_workers):\n","            batch_start = (len(dataset) // n_workers) * i\n","            if i == n_workers - 1:\n","                batch_end = len(dataset)\n","            else:\n","                batch_end = (len(dataset) // n_workers) * (i + 1)\n","            \n","            batch = dataset[batch_start: batch_end]\n","            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n","\n","        pool.close()\n","        pool.join()\n","\n","    processed = []\n","    for result in results:\n","        processed += result.get()\n","    return processed\n","\n","def preprocess_samples(dataset, word_dict):\n","    processed = []\n","    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n","        processed.append(preprocess_sample(sample[1], word_dict))\n","\n","    return processed\n","\n","def preprocess_sample(data, word_dict):\n","    processed = {}\n","    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n","    if 'Task 1' in data:\n","        processed['Label'] = [label_to_onehot(label) for label in data['Task 1'].split(' ')]\n","        \n","    return processed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27cfkeBhczoo","colab_type":"text"},"source":["## Data Packing Helpers"]},{"cell_type":"code","metadata":{"id":"OPrbrENnc_Y0","colab_type":"code","colab":{}},"source":["# Data Packing\n","class AbstractDataset(Dataset):\n","    def __init__(self, data, pad_idx, max_len = 500):\n","        self.data = data\n","        self.pad_idx = pad_idx\n","        self.max_len = max_len\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        return self.data[index]\n","        \n","    def collate_fn(self, datas):\n","        # get max length in this batch\n","        max_sent = max([len(data['Abstract']) for data in datas])\n","        max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n","        batch_abstract = []\n","        batch_label = []\n","        sent_len = []\n","        for data in datas:\n","            # padding abstract to make them in same length\n","            pad_abstract = []\n","            for sentence in data['Abstract']:\n","                if len(sentence) > max_len:\n","                    pad_abstract.append(sentence[:max_len])\n","                else:\n","                    pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n","            sent_len.append(len(pad_abstract))\n","            pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n","            batch_abstract.append(pad_abstract)\n","            # gather labels\n","            if 'Label' in data:\n","                pad_label = data['Label']\n","                pad_label.extend([[0]*6]*(max_sent-len(pad_label)))\n","                \n","                batch_label.append(pad_label)\n","        return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I019oD1wdPYL","colab_type":"text"},"source":["## Score Helpers"]},{"cell_type":"code","metadata":{"id":"zb1lzehwdRJA","colab_type":"code","colab":{}},"source":["# Score methods\n","class F1():\n","    def __init__(self):\n","        self.threshold = 0.4\n","        self.n_precision = 0\n","        self.n_recall = 0\n","        self.n_corrects = 0\n","        self.name = 'F1'\n","\n","    def reset(self):\n","        self.n_precision = 0\n","        self.n_recall = 0\n","        self.n_corrects = 0\n","\n","    def update(self, predicts, groundTruth):\n","        predicts = predicts > self.threshold\n","        self.n_precision += torch.sum(predicts).data.item()\n","        self.n_recall += torch.sum(groundTruth).data.item()\n","        self.n_corrects += torch.sum(groundTruth.type(torch.uint8) * predicts).data.item()\n","\n","    def get_score(self):\n","        recall = self.n_corrects / self.n_recall\n","        precision = self.n_corrects / (self.n_precision + 1e-20) #prevent divided by zero\n","        return 2 * (recall * precision) / (recall + precision + 1e-20)\n","\n","    def print_score(self):\n","        score = self.get_score()\n","        return '{:.5f}'.format(score)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YgzCBymYdUU1","colab_type":"text"},"source":["## Train Helpers"]},{"cell_type":"code","metadata":{"id":"vgDBaXBWdWBQ","colab_type":"code","colab":{}},"source":["# Train helpers\n","def _run_epoch(epoch, training):\n","    model.train(training)\n","    if training:\n","        description = 'Train'\n","        dataset = trainData\n","        shuffle = True\n","    else:\n","        description = 'Valid'\n","        dataset = validData\n","        shuffle = False\n","\n","    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                            shuffle=shuffle, collate_fn=dataset.collate_fn, num_workers=8)\n","\n","    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n","    loss = 0\n","    f1_score = F1()\n","    for i, (x, y, sent_len) in trange:\n","        o_labels, batch_loss = _run_iter(x,y)\n","        if training:\n","            opt.zero_grad()\n","            batch_loss.backward()\n","            opt.step()\n","\n","        loss += batch_loss.item()\n","        f1_score.update(o_labels.cpu(), y)\n","\n","        trange.set_postfix(\n","            loss=loss / (i + 1), f1=f1_score.print_score())\n","        del o_labels, batch_loss # Flush memory\n","\n","    if training:\n","        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n","    else:\n","        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n","    \n","    \n","\n","def _run_iter(x,y):\n","    abstract = x.to(device)\n","    labels = y.to(device)\n","    o_labels = model(abstract)\n","    l_loss = criteria(o_labels, labels)\n","    l_loss = (l_loss * sample_weights).mean()\n","    del abstract, labels  # flush\n","    return o_labels, l_loss\n","\n","def save(epoch):\n","    if not os.path.exists(program_url+'model'):\n","        os.makedirs(program_url+'model')\n","    torch.save(model.state_dict(), program_url+'model/model.pkl.'+str(epoch))\n","    with open(program_url+'model/history.json', 'w') as f:\n","        json.dump(history, f, indent=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6P0gvgcGccUz","colab_type":"text"},"source":["# Data-Preprocessing"]},{"cell_type":"code","metadata":{"id":"-CFrw3vKcL8Z","colab_type":"code","colab":{}},"source":["# Create a word dictionary\n","PAD_TOKEN = 0\n","UNK_TOKEN = 1\n","\n","if os.path.exists(program_url+'dicitonary.pkl'):\n","    with open(program_url+'dicitonary.pkl','rb') as f:\n","        word_dict = pickle.load(f)\n","else:\n","    words = set()\n","    words |= collect_words(dataset_url+'trainset.csv')\n","\n","    word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n","    for word in words:\n","        word_dict[word]=len(word_dict)\n","\n","    with open(program_url+'dicitonary.pkl','wb') as f:\n","        pickle.dump(word_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCKVyQCBcTCD","colab_type":"code","outputId":"2fad3cb7-012b-456f-9c55-3a2ddb9afb68","executionInfo":{"status":"ok","timestamp":1578550195486,"user_tz":-480,"elapsed":1579,"user":{"displayName":"NE6081022王士杰","photoUrl":"","userId":"16413260345102275028"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["print('[INFO] Start processing testset...')\n","test = get_dataset(dataset_url+'testset.csv', word_dict, n_workers=8)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[INFO] Start processing testset...\n"],"name":"stdout"},{"output_type":"stream","text":["0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","100%|██████████| 4/4 [00:00<00:00, 828.87it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"0wVYxFtfdDfO","colab_type":"code","colab":{}},"source":["testData = AbstractDataset(test, PAD_TOKEN, max_len = 128)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0mfO2ijdwt2n","colab_type":"text"},"source":["# Create GloVe Embedding vectors"]},{"cell_type":"code","metadata":{"id":"6SXvXzxTwwM2","colab_type":"code","colab":{}},"source":["glove_name = 'glove.840B.300d'\n","\n","if os.path.exists(program_url+'embedding_matrix_{}'.format(glove_name)):\n","    with open(program_url+'embedding_matrix_{}'.format(glove_name),'rb') as f:\n","        embedding_matrix = pickle.load(f)\n","else:\n","    # Parse the unzipped file (a .txt file) to build an index that maps \n","    # words (as strings) to their vector representation (as number vectors)\n","    wordvector_path = dataset_url+'glove/{}.txt'.format(glove_name)\n","    embeddings_index = {}\n","    f = open(wordvector_path, encoding='utf8')\n","    for line in f:\n","        values = line.split()\n","        word = ''.join(values[:-300])\n","        coefs = np.asarray(values[-300:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","    print('Found %s word vectors.' % len(embeddings_index))\n","\n","    # Preparing the GloVe word-embeddings matrix\n","    max_words = len(word_dict)\n","    embedding_matrix = np.zeros((max_words, embedding_dim))\n","    for word, i in word_dict.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","    embedding_matrix = torch.FloatTensor(embedding_matrix)\n","    with open(program_url+'embedding_matrix_{}'.format(glove_name),'wb') as f:\n","        pickle.dump(embedding_matrix, f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5ToPWNydJYR","colab_type":"text"},"source":["# Create Nets"]},{"cell_type":"code","metadata":{"id":"N_LwWGJ4dIvW","colab_type":"code","colab":{}},"source":["class GloVeNet(nn.Module):\n","    def __init__(self, vocabulary_size):\n","        super(GloVeNet, self).__init__()\n","        self.embedding_size = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.embedding = nn.Embedding(vocabulary_size, self.embedding_size)\n","        self.embedding.weight = torch.nn.Parameter(embedding_matrix)\n","        self.word_rnn = nn.GRU(self.embedding_size,\n","                                self.hidden_dim,\n","                                bidirectional=True,\n","                                batch_first=True)\n","        self.sent_rnn = nn.GRU(self.hidden_dim*2, self.hidden_dim, bidirectional=True, batch_first=True)\n","        \n","        # 1024 => 512\n","        self.l1 = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n","        torch.nn.init.kaiming_normal_(self.l1.weight)\n","        # 512 => 6\n","        self.l2 = nn.Linear(self.hidden_dim, 6)\n","    \n","\n","    def forward(self, x):\n","        # input 32, 11, 64\n","        x = self.embedding(x)  # emb 32, 11, 64, 300\n","        b,s,w,e = x.shape\n","        x = x.view(b,s*w,e)  # sent*word 32, 704, 300\n","        x, __ = self.word_rnn(x)  # rnn 32, 704, 1024\n","        x = x.view(b,s,w,-1)  # unwrap sw 32, 11, 64, 1024\n","        # 32 batch, 11 sentence, each 64 words\n","        x = torch.max(x,dim=2)[0]  # 32, 11, 1024\n","        x, __ = self.sent_rnn(x)  # 32, 11, 1024\n","        x = torch.relu(self.l1(x))  # 32, 11, 512\n","        x = torch.sigmoid(self.l2(x)) # 32, 11, 6\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0PoH_RKZdilU","colab_type":"text"},"source":["# Start Predicting"]},{"cell_type":"code","metadata":{"id":"6HbPZHu2dk1r","colab_type":"code","outputId":"5caa514c-7850-4687-a205-388ac75529f3","executionInfo":{"status":"ok","timestamp":1578550220202,"user_tz":-480,"elapsed":12386,"user":{"displayName":"NE6081022王士杰","photoUrl":"","userId":"16413260345102275028"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model = GloVeNet(len(word_dict))\n","opt = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n","criteria = torch.nn.BCELoss(reduction='none')\n","model.to(device)\n","\n","best_model = 709652\n","model.load_state_dict(state_dict=torch.load(os.path.join(program_url,'model/model.pkl.{}'.format(best_model))))\n","\n","# Use trained model to predict\n","model.train(False)\n","dataloader = DataLoader(dataset=testData,\n","                            batch_size=64,\n","                            shuffle=False,\n","                            collate_fn=testData.collate_fn,\n","                            num_workers=8)\n","trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n","prediction = []\n","for i, (x, y, sent_len) in trange:\n","    o_labels = model(x.to(device))\n","    result = o_labels>0.4\n","    for idx, o_label in enumerate(result):\n","        prediction.append(o_label[:sent_len[idx]].to('cpu'))\n","prediction = torch.cat(prediction).detach().numpy().astype(int)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Predict: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"WF1_Utvkd7eC","colab_type":"text"},"source":["## Submit Prediction Results"]},{"cell_type":"code","metadata":{"id":"qsoGKC-ehFel","colab_type":"code","outputId":"4f023859-5b1e-4e12-c8af-a0160ea75500","executionInfo":{"status":"ok","timestamp":1578550230441,"user_tz":-480,"elapsed":708,"user":{"displayName":"NE6081022王士杰","photoUrl":"","userId":"16413260345102275028"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["prediction"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 0, 0, 0, 0, 0],\n","       [0, 1, 1, 0, 0, 0],\n","       [0, 1, 0, 1, 0, 0],\n","       [0, 1, 0, 1, 0, 0]])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"no67fFvZhaSG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"540e8117-f9d4-4965-d15f-fbf142fd7aa8","executionInfo":{"status":"ok","timestamp":1578550572356,"user_tz":-480,"elapsed":704,"user":{"displayName":"NE6081022王士杰","photoUrl":"","userId":"16413260345102275028"}}},"source":["sent_text"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    With the emergence of mobile and wearable devi...\n","1    In order to maintain the retention rate and th...\n","2    We further verified the system with real data ...\n","3    In this way, we can know precisely about users...\n","dtype: object"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"uR8R2ANP8P8r","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}