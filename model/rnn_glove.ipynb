{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GloVe-709652.ipynb","provenance":[{"file_id":"1eHNY49IgBA6I8qSd8lipn5Un0BC2Mn3Y","timestamp":1577595886651},{"file_id":"1EBpL5Pn1gQpRjORtXIGmUzJyN4egCiHg","timestamp":1577289694523},{"file_id":"1SguLI5-mtrMhp2MnJh6QogUBLSqb1RtT","timestamp":1577251048121},{"file_id":"1Fd_Ux44Y1TUcpK851JjdTetKYOvGBhGN","timestamp":1577030337593},{"file_id":"1fFnkwPT6YVw54Qu7WH1rRj59W8lL5tbf","timestamp":1574738935690}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qB0MuUSRVC1b","colab_type":"text"},"source":["# Add Frameworks and Libraries"]},{"cell_type":"code","metadata":{"id":"rq96ri3_VE3Y","colab_type":"code","colab":{}},"source":["# Import libraries\n","import os\n","import pandas as pd\n","import pickle\n","import nltk\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import json\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_sample_weight\n","from multiprocessing import Pool\n","from nltk.tokenize import word_tokenize\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pdc-jMY6VUhA","colab_type":"text"},"source":["# Setup Environments"]},{"cell_type":"code","metadata":{"id":"YE87dQFJVZjB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1577706610131,"user_tz":-480,"elapsed":1821,"user":{"displayName":"NE6081022王士杰","photoUrl":"","userId":"16413260345102275028"}},"outputId":"cf429ed7-7d59-4543-cbbf-52a2b2292f7f"},"source":["# nltk, plt setup\n","nltk.download('punkt')\n","%matplotlib inline\n","\n","# drive setup\n","drive.mount('/content/drive')\n","dataset_url = \"/content/drive/My Drive/NCKUDMPH2/task1/dataset/\"\n","program_url = \"/content/drive/My Drive/NCKUDMPH2/task1/program/\"\n","\n","# setup gpu\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XvF2AOU2bd4X","colab_type":"text"},"source":["# Setup Hyperparameters"]},{"cell_type":"code","metadata":{"id":"d6AZ522ebdl6","colab_type":"code","colab":{}},"source":["embedding_dim = 300\n","hidden_dim = 512\n","learning_rate = 1e-4\n","max_epoch = 15\n","batch_size = 64"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G0QpN26BbkCy","colab_type":"text"},"source":["# Load Data and split it into Train/Dev/Test"]},{"cell_type":"code","metadata":{"id":"YIx_fSDIZZy7","colab_type":"code","colab":{}},"source":["dataset = pd.read_csv(dataset_url+'task1_trainset.csv', dtype=str)\n","\n","# Drop unuse columns\n","dataset.drop('Title',axis=1,inplace=True)\n","dataset.drop('Categories',axis=1,inplace=True)\n","dataset.drop('Created Date',axis=1, inplace=True)\n","dataset.drop('Authors',axis=1,inplace=True)\n","\n","# Data Partition\n","trainset, validset = train_test_split(dataset, test_size=0.1, random_state=2020)\n","\n","trainset.to_csv(dataset_url+'trainset.csv',index=False)\n","validset.to_csv(dataset_url+'validset.csv',index=False)\n","\n","# Test Data\n","dataset = pd.read_csv(dataset_url+'task1_private_testset.csv')\n","dataset.drop('Title',axis=1,inplace=True)\n","dataset.drop('Categories',axis=1,inplace=True)\n","dataset.drop('Created Date',axis=1, inplace=True)\n","dataset.drop('Authors',axis=1,inplace=True)\n","\n","dataset.to_csv(dataset_url+'testset.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"syVkcoWOcChU","colab_type":"text"},"source":["# Helpers"]},{"cell_type":"markdown","metadata":{"id":"O8IKI4m5cxb-","colab_type":"text"},"source":["## Tokenize Helpers"]},{"cell_type":"code","metadata":{"id":"AcAxVOhCbOLV","colab_type":"code","colab":{}},"source":["# tokenize the words\n","def collect_words(data_path, n_workers=4):\n","    df = pd.read_csv(data_path, dtype=str)\n","        \n","    sent_list = []\n","    for i in df.iterrows():\n","        sent_list += i[1]['Abstract'].split('$$$')\n","\n","    chunks = [\n","        ' '.join(sent_list[i:i + len(sent_list) // n_workers])\n","        for i in range(0, len(sent_list), len(sent_list) // n_workers)\n","    ]\n","    with Pool(n_workers) as pool:\n","        chunks = pool.map_async(word_tokenize, chunks)\n","        words = set(sum(chunks.get(), []))\n","\n","    return words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uLesPmucjZb","colab_type":"text"},"source":["## Data Formatting Helpers"]},{"cell_type":"code","metadata":{"id":"99_VaWlucllE","colab_type":"code","colab":{}},"source":["def label_to_onehot(labels):\n","    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n","    onehot = [0,0,0,0,0,0]\n","    for l in labels.split('/'):\n","        onehot[label_dict[l]] = 1\n","    return onehot\n","\n","def sentence_to_indices(sentence, word_dict):\n","    return [word_dict.get(word,UNK_TOKEN) for word in word_tokenize(sentence)]\n","\n","def get_dataset(data_path, word_dict, n_workers=4):\n","    dataset = pd.read_csv(data_path, dtype=str)\n","\n","    results = [None] * n_workers\n","    with Pool(processes=n_workers) as pool:\n","        for i in range(n_workers):\n","            batch_start = (len(dataset) // n_workers) * i\n","            if i == n_workers - 1:\n","                batch_end = len(dataset)\n","            else:\n","                batch_end = (len(dataset) // n_workers) * (i + 1)\n","            \n","            batch = dataset[batch_start: batch_end]\n","            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n","\n","        pool.close()\n","        pool.join()\n","\n","    processed = []\n","    for result in results:\n","        processed += result.get()\n","    return processed\n","\n","def preprocess_samples(dataset, word_dict):\n","    processed = []\n","    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n","        processed.append(preprocess_sample(sample[1], word_dict))\n","\n","    return processed\n","\n","def preprocess_sample(data, word_dict):\n","    processed = {}\n","    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n","    if 'Task 1' in data:\n","        processed['Label'] = [label_to_onehot(label) for label in data['Task 1'].split(' ')]\n","        \n","    return processed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27cfkeBhczoo","colab_type":"text"},"source":["## Data Packing Helpers"]},{"cell_type":"code","metadata":{"id":"OPrbrENnc_Y0","colab_type":"code","colab":{}},"source":["# Data Packing\n","class AbstractDataset(Dataset):\n","    def __init__(self, data, pad_idx, max_len = 500):\n","        self.data = data\n","        self.pad_idx = pad_idx\n","        self.max_len = max_len\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        return self.data[index]\n","        \n","    def collate_fn(self, datas):\n","        # get max length in this batch\n","        max_sent = max([len(data['Abstract']) for data in datas])\n","        max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n","        batch_abstract = []\n","        batch_label = []\n","        sent_len = []\n","        for data in datas:\n","            # padding abstract to make them in same length\n","            pad_abstract = []\n","            for sentence in data['Abstract']:\n","                if len(sentence) > max_len:\n","                    pad_abstract.append(sentence[:max_len])\n","                else:\n","                    pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n","            sent_len.append(len(pad_abstract))\n","            pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n","            batch_abstract.append(pad_abstract)\n","            # gather labels\n","            if 'Label' in data:\n","                pad_label = data['Label']\n","                pad_label.extend([[0]*6]*(max_sent-len(pad_label)))\n","                \n","                batch_label.append(pad_label)\n","        return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I019oD1wdPYL","colab_type":"text"},"source":["## Score Helpers"]},{"cell_type":"code","metadata":{"id":"zb1lzehwdRJA","colab_type":"code","colab":{}},"source":["# Score methods\n","class F1():\n","    def __init__(self):\n","        self.threshold = 0.4\n","        self.n_precision = 0\n","        self.n_recall = 0\n","        self.n_corrects = 0\n","        self.name = 'F1'\n","\n","    def reset(self):\n","        self.n_precision = 0\n","        self.n_recall = 0\n","        self.n_corrects = 0\n","\n","    def update(self, predicts, groundTruth):\n","        predicts = predicts > self.threshold\n","        self.n_precision += torch.sum(predicts).data.item()\n","        self.n_recall += torch.sum(groundTruth).data.item()\n","        self.n_corrects += torch.sum(groundTruth.type(torch.uint8) * predicts).data.item()\n","\n","    def get_score(self):\n","        recall = self.n_corrects / self.n_recall\n","        precision = self.n_corrects / (self.n_precision + 1e-20) #prevent divided by zero\n","        return 2 * (recall * precision) / (recall + precision + 1e-20)\n","\n","    def print_score(self):\n","        score = self.get_score()\n","        return '{:.5f}'.format(score)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YgzCBymYdUU1","colab_type":"text"},"source":["## Train Helpers"]},{"cell_type":"code","metadata":{"id":"vgDBaXBWdWBQ","colab_type":"code","colab":{}},"source":["# Train helpers\n","def _run_epoch(epoch, training):\n","    model.train(training)\n","    if training:\n","        description = 'Train'\n","        dataset = trainData\n","        shuffle = True\n","    else:\n","        description = 'Valid'\n","        dataset = validData\n","        shuffle = False\n","\n","    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                            shuffle=shuffle, collate_fn=dataset.collate_fn, num_workers=8)\n","\n","    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n","    loss = 0\n","    f1_score = F1()\n","    for i, (x, y, sent_len) in trange:\n","        o_labels, batch_loss = _run_iter(x,y)\n","        if training:\n","            opt.zero_grad()\n","            batch_loss.backward()\n","            opt.step()\n","\n","        loss += batch_loss.item()\n","        f1_score.update(o_labels.cpu(), y)\n","\n","        trange.set_postfix(\n","            loss=loss / (i + 1), f1=f1_score.print_score())\n","        del o_labels, batch_loss # Flush memory\n","\n","    if training:\n","        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n","    else:\n","        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n","    \n","    \n","\n","def _run_iter(x,y):\n","    abstract = x.to(device)\n","    labels = y.to(device)\n","    o_labels = model(abstract)\n","    l_loss = criteria(o_labels, labels)\n","    l_loss = (l_loss * sample_weights).mean()\n","    del abstract, labels  # flush\n","    return o_labels, l_loss\n","\n","def save(epoch):\n","    if not os.path.exists(program_url+'model'):\n","        os.makedirs(program_url+'model')\n","    torch.save(model.state_dict(), program_url+'model/model.pkl.'+str(epoch))\n","    with open(program_url+'model/history.json', 'w') as f:\n","        json.dump(history, f, indent=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YsqBsE8fdyrh","colab_type":"text"},"source":["## Submit Helpers"]},{"cell_type":"code","metadata":{"id":"SWmJz6LFd0i9","colab_type":"code","colab":{}},"source":["def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n","    sample = pd.read_csv(sampleFile)\n","    submit = {}\n","    submit['order_id'] = list(sample.order_id.values)\n","    redundant = len(sample) - prediction.shape[0]\n","    if public:\n","        submit['BACKGROUND'] = list(prediction[:,0]) + [0]*redundant\n","        submit['OBJECTIVES'] = list(prediction[:,1]) + [0]*redundant\n","        submit['METHODS'] = list(prediction[:,2]) + [0]*redundant\n","        submit['RESULTS'] = list(prediction[:,3]) + [0]*redundant\n","        submit['CONCLUSIONS'] = list(prediction[:,4]) + [0]*redundant\n","        submit['OTHERS'] = list(prediction[:,5]) + [0]*redundant\n","    else:\n","        submit['BACKGROUND'] = [0]*redundant + list(prediction[:,0])\n","        submit['OBJECTIVES'] = [0]*redundant + list(prediction[:,1])\n","        submit['METHODS'] = [0]*redundant + list(prediction[:,2])\n","        submit['RESULTS'] = [0]*redundant + list(prediction[:,3])\n","        submit['CONCLUSIONS'] = [0]*redundant + list(prediction[:,4])\n","        submit['OTHERS'] = [0]*redundant + list(prediction[:,5])\n","    df = pd.DataFrame.from_dict(submit) \n","    df.to_csv(filename,index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6P0gvgcGccUz","colab_type":"text"},"source":["# Data-Preprocessing"]},{"cell_type":"code","metadata":{"id":"-CFrw3vKcL8Z","colab_type":"code","colab":{}},"source":["# Create a word dictionary\n","PAD_TOKEN = 0\n","UNK_TOKEN = 1\n","\n","if os.path.exists(program_url+'dicitonary.pkl'):\n","    with open(program_url+'dicitonary.pkl','rb') as f:\n","        word_dict = pickle.load(f)\n","else:\n","    words = set()\n","    words |= collect_words(dataset_url+'trainset.csv')\n","\n","    word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n","    for word in words:\n","        word_dict[word]=len(word_dict)\n","\n","    with open(program_url+'dicitonary.pkl','wb') as f:\n","        pickle.dump(word_dict, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCKVyQCBcTCD","colab_type":"code","colab":{}},"source":["# Get 3 datasets\n","print('[INFO] Start processing trainset...')\n","train = get_dataset(dataset_url+'trainset.csv', word_dict, n_workers=8)\n","print('[INFO] Start processing validset...')\n","valid = get_dataset(dataset_url+'validset.csv', word_dict, n_workers=8)\n","print('[INFO] Start processing testset...')\n","test = get_dataset(dataset_url+'testset.csv', word_dict, n_workers=8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wVYxFtfdDfO","colab_type":"code","colab":{}},"source":["trainData = AbstractDataset(train, PAD_TOKEN, max_len = 128)\n","validData = AbstractDataset(valid, PAD_TOKEN, max_len = 128)\n","testData = AbstractDataset(test, PAD_TOKEN, max_len = 128)\n","del trainset, validset, dataset, train, valid, test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bzst0ty7zu66","colab_type":"text"},"source":["# handle data imbalance problem in training\n"]},{"cell_type":"code","metadata":{"id":"qmEREh2pzigf","colab_type":"code","colab":{}},"source":["def squeeze_sample_weights(sample_weights):\n","    sample_weights = np.array(sample_weights)\n","    return np.mean(sample_weights, axis=0)\n","\n","sample_weights = [compute_sample_weight(class_weight='balanced', y=abstract_labels) for data in trainData.data for abstract_labels in data['Label']]\n","sample_weights = np.array(sample_weights)\n","sample_weights = squeeze_sample_weights(sample_weights)\n","sample_weights = torch.tensor(sample_weights).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0mfO2ijdwt2n","colab_type":"text"},"source":["# Create GloVe Embedding vectors"]},{"cell_type":"code","metadata":{"id":"6SXvXzxTwwM2","colab_type":"code","colab":{}},"source":["glove_name = 'glove.840B.300d'\n","\n","if os.path.exists(program_url+'embedding_matrix_{}'.format(glove_name)):\n","    with open(program_url+'embedding_matrix_{}'.format(glove_name),'rb') as f:\n","        embedding_matrix = pickle.load(f)\n","else:\n","    # Parse the unzipped file (a .txt file) to build an index that maps \n","    # words (as strings) to their vector representation (as number vectors)\n","    wordvector_path = dataset_url+'glove/{}.txt'.format(glove_name)\n","    embeddings_index = {}\n","    f = open(wordvector_path, encoding='utf8')\n","    for line in f:\n","        values = line.split()\n","        word = ''.join(values[:-300])\n","        coefs = np.asarray(values[-300:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","    print('Found %s word vectors.' % len(embeddings_index))\n","\n","    # Preparing the GloVe word-embeddings matrix\n","    max_words = len(word_dict)\n","    embedding_matrix = np.zeros((max_words, embedding_dim))\n","    for word, i in word_dict.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","    embedding_matrix = torch.FloatTensor(embedding_matrix)\n","    with open(program_url+'embedding_matrix_{}'.format(glove_name),'wb') as f:\n","        pickle.dump(embedding_matrix, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5ToPWNydJYR","colab_type":"text"},"source":["# Create Nets"]},{"cell_type":"code","metadata":{"id":"N_LwWGJ4dIvW","colab_type":"code","colab":{}},"source":["class GloVeNet(nn.Module):\n","    def __init__(self, vocabulary_size):\n","        super(GloVeNet, self).__init__()\n","        self.embedding_size = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.embedding = nn.Embedding(vocabulary_size, self.embedding_size)\n","        self.embedding.weight = torch.nn.Parameter(embedding_matrix)\n","        self.word_rnn = nn.GRU(self.embedding_size,\n","                                self.hidden_dim,\n","                                bidirectional=True,\n","                                batch_first=True)\n","        self.sent_rnn = nn.GRU(self.hidden_dim*2, self.hidden_dim, bidirectional=True, batch_first=True)\n","        \n","        # 1024 => 512\n","        self.l1 = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n","        torch.nn.init.kaiming_normal_(self.l1.weight)\n","        # 512 => 6\n","        self.l2 = nn.Linear(self.hidden_dim, 6)\n","    \n","\n","    def forward(self, x):\n","        # input 32, 11, 64\n","        x = self.embedding(x)  # emb 32, 11, 64, 300\n","        b,s,w,e = x.shape\n","        x = x.view(b,s*w,e)  # sent*word 32, 704, 300\n","        x, __ = self.word_rnn(x)  # rnn 32, 704, 1024\n","        x = x.view(b,s,w,-1)  # unwrap sw 32, 11, 64, 1024\n","        # 32 batch, 11 sentence, each 64 words\n","        x = torch.max(x,dim=2)[0]  # 32, 11, 1024\n","        x, __ = self.sent_rnn(x)  # 32, 11, 1024\n","        x = torch.relu(self.l1(x))  # 32, 11, 512\n","        x = torch.sigmoid(self.l2(x)) # 32, 11, 6\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPjoGTEndeRQ","colab_type":"text"},"source":["# Start Learning"]},{"cell_type":"code","metadata":{"id":"voROfvm1dfdg","colab_type":"code","colab":{}},"source":["model = GloVeNet(len(word_dict))\n","opt = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n","criteria = torch.nn.BCELoss(reduction='none')\n","model.to(device)\n","history = {'train':[],'valid':[]}\n","\n","for epoch in range(max_epoch):\n","    print('Epoch: {}'.format(epoch))\n","    _run_epoch(epoch, True)\n","    _run_epoch(epoch, False)\n","    save(epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s8gY4ZfwdnM6","colab_type":"text"},"source":["## Plot Learning Result"]},{"cell_type":"code","metadata":{"id":"l_15bNM6dqoW","colab_type":"code","colab":{}},"source":["with open(program_url+'model/history.json', 'r') as f:\n","    history = json.loads(f.read())\n","    \n","train_loss = [l['loss'] for l in history['train']]\n","valid_loss = [l['loss'] for l in history['valid']]\n","train_f1 = [l['f1'] for l in history['train']]\n","valid_f1 = [l['f1'] for l in history['valid']]\n","\n","plt.figure(figsize=(7,5))\n","plt.title('Loss')\n","plt.plot(train_loss, label='train')\n","plt.plot(valid_loss, label='valid')\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(7,5))\n","plt.title('F1 Score')\n","plt.plot(train_f1, label='train')\n","plt.plot(valid_f1, label='valid')\n","plt.legend()\n","plt.show()\n","\n","print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0PoH_RKZdilU","colab_type":"text"},"source":["# Start Predicting"]},{"cell_type":"code","metadata":{"id":"6HbPZHu2dk1r","colab_type":"code","colab":{}},"source":["best_model = 709652\n","model.load_state_dict(state_dict=torch.load(os.path.join(program_url,'model/model.pkl.{}'.format(best_model))))\n","_run_epoch(1, False)\n","\n","# Use trained model to predict\n","model.train(False)\n","dataloader = DataLoader(dataset=testData,\n","                            batch_size=64,\n","                            shuffle=False,\n","                            collate_fn=testData.collate_fn,\n","                            num_workers=8)\n","trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n","prediction = []\n","for i, (x, y, sent_len) in trange:\n","    o_labels = model(x.to(device))\n","    result = o_labels>0.4\n","    for idx, o_label in enumerate(result):\n","        prediction.append(o_label[:sent_len[idx]].to('cpu'))\n","prediction = torch.cat(prediction).detach().numpy().astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WF1_Utvkd7eC","colab_type":"text"},"source":["## Submit Prediction Results"]},{"cell_type":"code","metadata":{"id":"sRZ95_HJd7LB","colab_type":"code","colab":{}},"source":["# SubmitGenerator(prediction,\n","#                 dataset_url+'task1_sample_submission.csv', \n","#                 True, \n","#                 dataset_url+'task1_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vU-Au9ArW_C","colab_type":"code","colab":{}},"source":["SubmitGenerator(prediction,\n","                dataset_url+'task1_sample_submission.csv', \n","                False,\n","                dataset_url+'task1_submission.csv')"],"execution_count":null,"outputs":[]}]}