{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BERT-Improvement-abstract.ipynb","provenance":[{"file_id":"1hsjwNJ43GTNV0EYCcPAZGPdqKUQVp7lI","timestamp":1575991435570},{"file_id":"1Fd_Ux44Y1TUcpK851JjdTetKYOvGBhGN","timestamp":1574918810026},{"file_id":"1fFnkwPT6YVw54Qu7WH1rRj59W8lL5tbf","timestamp":1574738935690}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qB0MuUSRVC1b"},"source":["# Import Frameworks"]},{"cell_type":"code","metadata":{"id":"osllw4OY_4bV","colab_type":"code","outputId":"400b3b7a-aab2-47c9-8828-e9d7263ea8ab","executionInfo":{"status":"ok","timestamp":1576344735762,"user_tz":-480,"elapsed":1076,"user":{"displayName":"NE6081022王士杰","photoUrl":"","userId":"16413260345102275028"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile setup.sh\n","git clone https://github.com/NVIDIA/apex\n","cd apex\n","pip install -v --no-cache-dir ./"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rq96ri3_VE3Y","colab":{}},"source":["!sh setup.sh\n","!pip install transformers\n","\n","# Import frameworks\n","import os\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import torch\n","import matplotlib.pyplot as plt\n","import json\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from google.colab import drive\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from IPython.display import clear_output\n","\n","clear_output()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Pdc-jMY6VUhA"},"source":["# Setup Environments"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YE87dQFJVZjB","colab":{}},"source":["# plt setup\n","%matplotlib inline\n","\n","# drive setup\n","drive.mount('/content/drive')\n","dataset_url = \"/content/drive/My Drive/NCKUDMPH2/task1/dataset/\"\n","program_url = \"/content/drive/My Drive/NCKUDMPH2/task1/program/\"\n","\n","# root = os.getcwd()\n","# dataset_url = os.path.join(root, \"dataset/\")\n","# program_url = os.path.join(root, \"program/\")\n","\n","# setup gpu\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# setup parameters\n","num_labels = 6\n","batch_size = 4\n","epochs = 5\n","learning_rate = 2e-5\n","adam_epsilon = 1e-8\n","\n","clear_output()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TCiAw0iiJz58"},"source":["# Setup BERT"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zdrdqryuJzP0","colab":{}},"source":["PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n","model = BertForSequenceClassification.from_pretrained(\n","    PRETRAINED_MODEL_NAME, num_labels=num_labels)\n","\n","clear_output()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G0QpN26BbkCy"},"source":["# Load Data and split it into Train/Dev/Test"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YIx_fSDIZZy7","colab":{}},"source":["dataset = pd.read_csv(dataset_url+'task1_trainset.csv', dtype=str)\n","\n","# Drop unuse columns\n","dataset.drop('Title',axis=1,inplace=True)\n","dataset.drop('Categories',axis=1,inplace=True)\n","dataset.drop('Created Date',axis=1, inplace=True)\n","dataset.drop('Authors',axis=1,inplace=True)\n","\n","# for i in range(len(dataset['Abstract'])):\n","#     dataset['Abstract'][i] = remove_stopwords(dataset['Abstract'][i])\n","\n","# Data Partition\n","trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n","\n","trainset.to_csv(dataset_url+'trainset.csv',index=False)\n","validset.to_csv(dataset_url+'validset.csv',index=False)\n","\n","# Test Data\n","testset = pd.read_csv(dataset_url+'task1_public_testset.csv')\n","testset.drop('Title',axis=1,inplace=True)\n","testset.drop('Categories',axis=1,inplace=True)\n","testset.drop('Created Date',axis=1, inplace=True)\n","testset.drop('Authors',axis=1,inplace=True)\n","\n","# for i in range(len(dataset['Abstract'])):\n","#     dataset['Abstract'][i] = remove_stopwords(dataset['Abstract'][i])\n","\n","testset.to_csv(dataset_url+'testset.csv',index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BI_AJqg_chV","colab_type":"code","colab":{}},"source":["# tasks = pd.Series(trainset['Task 1']).to_list()\n","# tasks = [t for task in tasks for t in task.split(\" \")]\n","# pd.Series(tasks).value_counts(normalize=True)\n","# clear_output()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_0AYh5VmxczO"},"source":["# Parse Dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YlDk9O5ks2Bt","colab":{}},"source":["def label_to_onehot(labels):\n","    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n","    onehot = [0,0,0,0,0,0]\n","    for l in labels.split('/'):\n","        onehot[label_dict[l]] = 1\n","    return onehot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y5U8vr2MdU3F","colab":{}},"source":["def label_to_ids_list(sentence):\n","    new_sent = \"[CLS] \" + sentence.lower() + \" [SEP]\"\n","    ids = [tokenizer.convert_tokens_to_ids(text) for text in tokenizer.tokenize(new_sent)]\n","    return ids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qkC399W_chc","colab_type":"code","colab":{}},"source":["def get_padding_item(item):\n","    return pad_sequences(item,\n","                         maxlen=64, dtype=\"long\",\n","                         truncating=\"post\", padding=\"post\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"77JufjOkxiNG","colab":{}},"source":["def generate_masks(input_ids):\n","    # Create attention masks\n","    attention_masks = []\n","    # Create a mask of 1s for each token followed by 0s for padding\n","    for seq in input_ids:\n","        seq_mask = [float(i > 0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return attention_masks"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cdHQzA9zxiyO","colab":{}},"source":["# sent_list += [tokenizer.encode(abstract, add_special_tokens=True)\n","#               for abstract in abstract_parts]\n","\n","# split => 每個句子sent_list, label_list, 產生對應每個句子 mask\n","# 封裝成 1 個 abstract\n","# abstract : 32*64, label : 32*6\n","# 一個 batch 就會有 32*32*64 => 訓練時轉成 1024*64\n","\n","def generate_dataloader(dataset, labeling, sequntial):\n","    x, m, y = [], [], []\n","    for i in dataset.iterrows():\n","        abstract_parts = i[1]['Abstract'].split('$$$')\n","        sent_id_list = get_padding_item([label_to_ids_list(abstract) for abstract in abstract_parts])\n","        pad_abstract = np.pad(sent_id_list, ((0, 32-len(sent_id_list)), (0,0)), 'constant')\n","        x.append(torch.tensor(pad_abstract))\n","        \n","        attention_masks = generate_masks(pad_abstract)\n","        m.append(attention_masks)\n","        if labeling:\n","            label_parts = i[1]['Task 1'].split(\" \")\n","            label_list = [label_to_onehot(label) for label in label_parts]\n","            pad_labels = np.pad(label_list, ((0, 32-len(label_list)), (0,0)), 'constant')\n","            y.append(pad_labels)\n","    \n","    inputs = torch.stack(x)\n","    masks = torch.tensor(m)\n","    \n","    if labeling:\n","        labels = torch.tensor(y)\n","\n","    dataset = TensorDataset(\n","        inputs, masks, labels) if labeling else TensorDataset(inputs, masks)\n","    sampler = SequentialSampler(\n","        dataset) if sequntial else RandomSampler(dataset)\n","    return DataLoader(dataset, sampler=sampler,\n","                      batch_size=batch_size, num_workers=0, drop_last=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MSNrK8SvsRCu","scrolled":true,"colab":{}},"source":["if os.path.exists(program_url+'train_dataloader_{}'.format(batch_size)):\n","    with open(program_url+'train_dataloader_{}'.format(batch_size), 'rb') as f:\n","        train_dataloader = pickle.load(f)\n","    with open(program_url+'valid_dataloader_{}'.format(batch_size), 'rb') as f:\n","        valid_dataloader = pickle.load(f)\n","    with open(program_url+'test_dataloader_{}'.format(batch_size), 'rb') as f:\n","        test_dataloader = pickle.load(f)\n","else:\n","    train_dataloader = generate_dataloader(trainset, True, False)\n","    with open(program_url+'train_dataloader_{}'.format(batch_size), 'wb') as f:\n","        pickle.dump(train_dataloader, f)\n","    valid_dataloader = generate_dataloader(validset, True, True)\n","    with open(program_url+'valid_dataloader_{}'.format(batch_size), 'wb') as f:\n","        pickle.dump(valid_dataloader, f)\n","    test_dataloader = generate_dataloader(testset, False, True)\n","    with open(program_url+'test_dataloader_{}'.format(batch_size), 'wb') as f:\n","        pickle.dump(test_dataloader, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQsIdKuw_chj","colab_type":"code","colab":{}},"source":["del dataset, trainset, validset, testset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FU1cKKZBxsCN"},"source":["# Start Training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ho1JJ9WBxyv6","colab":{}},"source":["class F1():\n","    def __init__(self):\n","        self.threshold = 0.5\n","        self.n_precision = 0\n","        self.n_recall = 0\n","        self.n_corrects = 0\n","        self.name = 'F1'\n","\n","    def reset(self):\n","        self.n_precision = 0\n","        self.n_recall = 0\n","        self.n_corrects = 0\n","\n","    def update(self, predicts, groundTruth):\n","        predicts = predicts > self.threshold\n","        self.n_precision += torch.sum(predicts).data.item()\n","        self.n_recall += torch.sum(groundTruth).data.item()\n","        self.n_corrects += torch.sum(groundTruth.type(torch.uint8)\n","                                     * predicts).data.item()\n","\n","    def get_score(self):\n","        recall = self.n_corrects / self.n_recall\n","        precision = self.n_corrects / \\\n","            (self.n_precision + 1e-20)  # prevent divided by zero\n","        return 2 * (recall * precision) / (recall + precision + 1e-20)\n","\n","    def print_score(self):\n","        score = self.get_score()\n","        return '{:.5f}'.format(score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Kpe9LO73x1aE","colab":{}},"source":["def run_epoch(epoch, isTraining):\n","    if isTraining:\n","        model.train()\n","    else:\n","        model.eval()\n","\n","    f1_score = F1()\n","    loss = 0\n","    dataloader = train_dataloader if isTraining else valid_dataloader\n","    desc = \"Train\" if isTraining else \"Valid\"\n","\n","    trange = tqdm(enumerate(dataloader), total=len(\n","        dataloader), desc=desc)\n","\n","    for i, batch in trange:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        b_input_ids = b_input_ids.long().view(batch_size*32, -1)\n","        b_input_mask = b_input_mask.view(batch_size*32, -1)\n","        b_labels = b_labels.float().view(batch_size*32, -1).squeeze()\n","\n","        # Extract tensor from tuple\n","        if isTraining:\n","            optimizer.zero_grad()\n","            o_labels = model(b_input_ids, b_input_mask)[0]\n","            batch_loss = criteria(o_labels, b_labels)\n","        else:\n","            with torch.no_grad(): \n","                o_labels = model(b_input_ids, b_input_mask)[0]\n","                batch_loss = criteria(o_labels, b_labels)\n","        \n","        if isTraining:\n","            with amp.scale_loss(batch_loss, optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","            optimizer.step()\n","            del scaled_loss\n","            \n","        loss += float(batch_loss.item())\n","        f1_score.update(o_labels, b_labels)\n","        trange.set_postfix(\n","            loss=loss / (i + 1), f1=f1_score.print_score())\n","        history['train' if isTraining else 'valid'].append(\n","            {'f1': f1_score.get_score(), 'loss': loss / len(trange)})\n","        \n","        # flush memory\n","        del o_labels, b_input_ids, b_labels, b_input_mask, batch_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KsOsf0Dex4wz","colab":{}},"source":["def save(epoch):\n","    if not os.path.exists(program_url+'model'):\n","        os.makedirs(program_url+'model')\n","    torch.save(model.state_dict(), program_url+'model/model.pkl.'+str(epoch))\n","    with open(program_url+'model/history.json', 'w') as f:\n","        json.dump(history, f, indent=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"error","timestamp":1576344143418,"user_tz":-480,"elapsed":28815,"user":{"displayName":"NE6081022王士杰","photoUrl":"","userId":"16413260345102275028"}},"id":"SFmYU-mux-E8","outputId":"059cfcba-89ed-473c-d5ef-345aee9a7eb4","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["from apex import amp\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n","\n","# BCELoss can't handle negative values\n","criteria = torch.nn.BCEWithLogitsLoss()\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n","model.to(device)\n","\n","model, optimizer = amp.initialize(model, optimizer)\n","\n","history = {'train': [], 'valid': []}\n","train_loss_set = []\n","\n","for epoch in range(epochs):\n","    run_epoch(epoch, True)\n","    run_epoch(epoch, False)\n","    save(epoch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\rTrain:   0%|          | 0/1575 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"],"name":"stdout"},{"output_type":"stream","text":["Train: 100%|██████████| 1575/1575 [2:45:27<00:00,  6.30s/it, f1=0.46987, loss=0.0831]\n","Valid: 100%|██████████| 175/175 [06:12<00:00,  2.12s/it, f1=0.59472, loss=0.0654]\n","Train: 100%|██████████| 1575/1575 [2:45:18<00:00,  6.30s/it, f1=0.61768, loss=0.0593]\n","Valid: 100%|██████████| 175/175 [06:12<00:00,  2.13s/it, f1=0.59829, loss=0.0636]\n","Train: 100%|██████████| 1575/1575 [2:45:14<00:00,  6.30s/it, f1=0.67961, loss=0.0523]\n","Valid: 100%|██████████| 175/175 [06:12<00:00,  2.13s/it, f1=0.61316, loss=0.0666]\n","Train:  42%|████▏     | 656/1575 [1:08:51<1:36:30,  6.30s/it, f1=0.73766, loss=0.0452]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a9GRAFvmyCiq"},"source":["# Ploting the Results"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K3GJfCjZyEa-","colab":{}},"source":["with open(program_url+'model/history.json', 'r') as f:\n","    history = json.loads(f.read())\n","\n","train_loss = [l['loss'] for l in history['train']]\n","valid_loss = [l['loss'] for l in history['valid']]\n","train_f1 = [l['f1'] for l in history['train']]\n","valid_f1 = [l['f1'] for l in history['valid']]\n","\n","plt.figure(figsize=(15, 8))\n","plt.title('Loss')\n","plt.plot(train_loss, label='train')\n","plt.plot(valid_loss, label='valid')\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(15, 8))\n","plt.title('F1 Score')\n","plt.plot(train_f1, label='train')\n","plt.plot(valid_f1, label='valid')\n","plt.legend()\n","plt.show()\n","\n","print('Best F1 score ', max([[l['f1'], idx]\n","                             for idx, l in enumerate(history['valid'])]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gv1Zeya8yJ8U"},"source":["# Prediction"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C_EjJsyWyLw3","colab":{}},"source":["def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n","    sample = pd.read_csv(sampleFile)\n","    submit = {}\n","    submit['order_id'] = list(sample.order_id.values)\n","    redundant = len(sample) - prediction.shape[0]\n","    if public:\n","        submit['BACKGROUND'] = list(prediction[:,0]) + [0]*redundant\n","        submit['OBJECTIVES'] = list(prediction[:,1]) + [0]*redundant\n","        submit['METHODS'] = list(prediction[:,2]) + [0]*redundant\n","        submit['RESULTS'] = list(prediction[:,3]) + [0]*redundant\n","        submit['CONCLUSIONS'] = list(prediction[:,4]) + [0]*redundant\n","        submit['OTHERS'] = list(prediction[:,5]) + [0]*redundant\n","    else:\n","        submit['BACKGROUND'] = [0]*redundant + list(prediction[:,0])\n","        submit['OBJECTIVES'] = [0]*redundant + list(prediction[:,1])\n","        submit['METHODS'] = [0]*redundant + list(prediction[:,2])\n","        submit['RESULTS'] = [0]*redundant + list(prediction[:,3])\n","        submit['CONCLUSIONS'] = [0]*redundant + list(prediction[:,4])\n","        submit['OTHERS'] = [0]*redundant + list(prediction[:,5])\n","    df = pd.DataFrame.from_dict(submit) \n","    df.to_csv(filename,index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lpboVQoQyI7I","colab":{}},"source":["# model.train(False)\n","# trange = tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc='Test')\n","# prediction = []  # prediction will be a (131166, 6) list\n","\n","# for i, batch in trange:\n","#     batch = tuple(t.to(device) for t in batch)\n","#     b_input_ids, b_input_mask = batch\n","#     b_input_ids = b_input_ids.long()\n","#     with torch.no_grad():\n","#         o_labels = model(b_input_ids, b_input_mask)[0]\n","#     o_labels = o_labels > 0.5\n","#     batch_result = np.where(o_labels.cpu().numpy(), 1, 0)  # convert True/False into 1/0\n","#     prediction.extend(batch_result)\n","# prediction = np.array(prediction)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WF1_Utvkd7eC"},"source":["# Submit Prediction Results"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sRZ95_HJd7LB","colab":{}},"source":["# SubmitGenerator(prediction,\n","#                 dataset_url+'task1_sample_submission.csv', \n","#                 True, \n","#                 dataset_url+'task1_submission.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sWrboJsfsRDF","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}